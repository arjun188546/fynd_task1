{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Task 1: Rating Prediction via Prompting\n",
                "## Fynd AI Intern Assessment 2.0\n",
                "\n",
                "This notebook implements and evaluates **4 different prompting approaches** to predict star ratings (1-5) from Yelp reviews.\n",
                "\n",
                "### Approaches:\n",
                "1. **Simple Direct Prompting** - Baseline with minimal context\n",
                "2. **Few-Shot Learning** - Includes examples to guide the model\n",
                "3. **Chain-of-Thought (CoT)** - Step-by-step reasoning\n",
                "4. **Structured Criteria-Based Analysis** - Detailed evaluation framework"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Setup and Imports"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Install required packages (run once)\n",
                "# !pip install pandas numpy scikit-learn google-generativeai python-dotenv matplotlib seaborn"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import pandas as pd\n",
                "import numpy as np\n",
                "import json\n",
                "import time\n",
                "import re\n",
                "from typing import Dict, List, Tuple, Optional\n",
                "from dotenv import load_dotenv\n",
                "import os\n",
                "import google.generativeai as genai\n",
                "from sklearn.metrics import accuracy_score, mean_absolute_error, classification_report, confusion_matrix\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "from collections import Counter\n",
                "\n",
                "# Set style for visualizations\n",
                "plt.style.use('seaborn-v0_8-darkgrid')\n",
                "sns.set_palette(\"husl\")\n",
                "\n",
                "print(\"✓ All imports successful\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Configuration and API Setup"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load environment variables\n",
                "load_dotenv()\n",
                "\n",
                "# Configure Gemini API\n",
                "GEMINI_API_KEY = os.getenv('GEMINI_API_KEY')\n",
                "\n",
                "if not GEMINI_API_KEY or GEMINI_API_KEY == 'your_api_key_here':\n",
                "    raise ValueError(\n",
                "        \"❌ GEMINI_API_KEY not found!\\n\"\n",
                "        \"Please create a .env file with your API key:\\n\"\n",
                "        \"GEMINI_API_KEY=your_actual_key_here\\n\\n\"\n",
                "        \"Get your free key from: https://makersuite.google.com/app/apikey\"\n",
                "    )\n",
                "\n",
                "genai.configure(api_key=GEMINI_API_KEY)\n",
                "\n",
                "# Initialize model\n",
                "model = genai.GenerativeModel('gemini-2.5-flash')\n",
                "\n",
                "print(\"✓ Gemini API configured successfully\")\n",
                "print(f\"✓ Using model: gemini-2.5-flash\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Load and Sample Dataset"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load dataset\n",
                "df = pd.read_csv('yelp.csv')\n",
                "\n",
                "print(f\"Total reviews in dataset: {len(df):,}\")\n",
                "print(f\"\\nColumns: {df.columns.tolist()}\")\n",
                "print(f\"\\nFirst few rows:\")\n",
                "df.head()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Check rating distribution\n",
                "print(\"Rating distribution in full dataset:\")\n",
                "print(df['stars'].value_counts().sort_index())\n",
                "\n",
                "plt.figure(figsize=(10, 5))\n",
                "df['stars'].value_counts().sort_index().plot(kind='bar', color='skyblue', edgecolor='black')\n",
                "plt.title('Rating Distribution in Full Dataset', fontsize=14, fontweight='bold')\n",
                "plt.xlabel('Star Rating')\n",
                "plt.ylabel('Count')\n",
                "plt.xticks(rotation=0)\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Stratified sampling to maintain rating distribution\n",
                "SAMPLE_SIZE = 200\n",
                "np.random.seed(42)  # For reproducibility\n",
                "\n",
                "# Sample proportionally from each rating\n",
                "sample_df = df.groupby('stars', group_keys=False).apply(\n",
                "    lambda x: x.sample(min(len(x), int(SAMPLE_SIZE * len(x) / len(df))), random_state=42)\n",
                ").reset_index(drop=True)\n",
                "\n",
                "# If we don't have exactly SAMPLE_SIZE, adjust\n",
                "if len(sample_df) < SAMPLE_SIZE:\n",
                "    additional = df[~df.index.isin(sample_df.index)].sample(\n",
                "        SAMPLE_SIZE - len(sample_df), random_state=42\n",
                "    )\n",
                "    sample_df = pd.concat([sample_df, additional]).reset_index(drop=True)\n",
                "elif len(sample_df) > SAMPLE_SIZE:\n",
                "    sample_df = sample_df.sample(SAMPLE_SIZE, random_state=42).reset_index(drop=True)\n",
                "\n",
                "print(f\"\\n✓ Sampled {len(sample_df)} reviews\")\n",
                "print(f\"\\nSample rating distribution:\")\n",
                "print(sample_df['stars'].value_counts().sort_index())\n",
                "\n",
                "# Visualize sample distribution\n",
                "plt.figure(figsize=(10, 5))\n",
                "sample_df['stars'].value_counts().sort_index().plot(kind='bar', color='lightcoral', edgecolor='black')\n",
                "plt.title('Rating Distribution in Sample', fontsize=14, fontweight='bold')\n",
                "plt.xlabel('Star Rating')\n",
                "plt.ylabel('Count')\n",
                "plt.xticks(rotation=0)\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Prompting Approaches\n",
                "\n",
                "### Approach 1: Simple Direct Prompting\n",
                "**Strategy**: Minimal context, direct instruction\n",
                "**Rationale**: Establishes baseline performance without any sophisticated techniques"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def create_simple_prompt(review_text: str) -> str:\n",
                "    \"\"\"Simple direct prompting approach.\"\"\"\n",
                "    return f\"\"\"Analyze this Yelp review and predict the star rating (1-5).\n",
                "\n",
                "Review: \"{review_text}\"\n",
                "\n",
                "Return ONLY a JSON object with this exact format:\n",
                "{{\n",
                "  \"predicted_stars\": <number between 1-5>,\n",
                "  \"explanation\": \"<brief reasoning>\"\n",
                "}}\n",
                "\"\"\"\n",
                "\n",
                "# Test the prompt\n",
                "test_review = sample_df.iloc[0]['text']\n",
                "print(\"Example Prompt (Approach 1):\")\n",
                "print(\"=\"*80)\n",
                "print(create_simple_prompt(test_review))\n",
                "print(\"=\"*80)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Approach 2: Few-Shot Learning\n",
                "**Strategy**: Provide 5 diverse examples to calibrate the model\n",
                "**Rationale**: Research shows examples improve consistency and accuracy"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def create_fewshot_prompt(review_text: str) -> str:\n",
                "    \"\"\"Few-shot learning approach with examples.\"\"\"\n",
                "    return f\"\"\"You are an expert at analyzing Yelp reviews and predicting star ratings.\n",
                "\n",
                "Here are some examples:\n",
                "\n",
                "Example 1:\n",
                "Review: \"Absolutely amazing! Best food I've ever had. Service was impeccable and the atmosphere was perfect.\"\n",
                "Rating: 5 stars\n",
                "Reasoning: Extremely positive language, multiple superlatives, all aspects praised.\n",
                "\n",
                "Example 2:\n",
                "Review: \"Pretty good experience overall. Food was tasty but service was a bit slow.\"\n",
                "Rating: 4 stars\n",
                "Reasoning: Positive but with minor criticism, balanced review.\n",
                "\n",
                "Example 3:\n",
                "Review: \"It was okay. Nothing special but nothing terrible either. Average food, average service.\"\n",
                "Rating: 3 stars\n",
                "Reasoning: Neutral language, no strong positives or negatives, mediocre experience.\n",
                "\n",
                "Example 4:\n",
                "Review: \"Disappointed. Food was cold and service was inattentive. Probably won't return.\"\n",
                "Rating: 2 stars\n",
                "Reasoning: Multiple negative points, clear dissatisfaction, unlikely to return.\n",
                "\n",
                "Example 5:\n",
                "Review: \"Terrible experience. Rude staff, disgusting food, dirty restaurant. Avoid at all costs!\"\n",
                "Rating: 1 star\n",
                "Reasoning: Extremely negative, multiple severe complaints, strong warning to others.\n",
                "\n",
                "Now analyze this review:\n",
                "Review: \"{review_text}\"\n",
                "\n",
                "Return ONLY a JSON object:\n",
                "{{\n",
                "  \"predicted_stars\": <number between 1-5>,\n",
                "  \"explanation\": \"<brief reasoning>\"\n",
                "}}\n",
                "\"\"\"\n",
                "\n",
                "# Test the prompt\n",
                "print(\"Example Prompt (Approach 2):\")\n",
                "print(\"=\"*80)\n",
                "print(create_fewshot_prompt(test_review)[:500] + \"...\")\n",
                "print(\"=\"*80)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Approach 3: Chain-of-Thought (CoT)\n",
                "**Strategy**: Ask model to reason step-by-step before deciding\n",
                "**Rationale**: Improves performance on complex reasoning tasks"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def create_cot_prompt(review_text: str) -> str:\n",
                "    \"\"\"Chain-of-thought prompting approach.\"\"\"\n",
                "    return f\"\"\"Analyze this Yelp review step-by-step to predict the star rating.\n",
                "\n",
                "Review: \"{review_text}\"\n",
                "\n",
                "Think through this systematically:\n",
                "1. First, identify the overall sentiment (very negative, negative, neutral, positive, very positive)\n",
                "2. Note specific positive aspects mentioned\n",
                "3. Note specific negative aspects or complaints\n",
                "4. Consider the intensity of language used\n",
                "5. Determine if the reviewer would recommend this place\n",
                "6. Based on all factors, assign a rating from 1-5 stars\n",
                "\n",
                "Return ONLY a JSON object:\n",
                "{{\n",
                "  \"predicted_stars\": <number between 1-5>,\n",
                "  \"explanation\": \"<your step-by-step reasoning>\"\n",
                "}}\n",
                "\"\"\"\n",
                "\n",
                "# Test the prompt\n",
                "print(\"Example Prompt (Approach 3):\")\n",
                "print(\"=\"*80)\n",
                "print(create_cot_prompt(test_review))\n",
                "print(\"=\"*80)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Approach 4: Structured Criteria-Based Analysis\n",
                "**Strategy**: Detailed evaluation framework with explicit criteria\n",
                "**Rationale**: Mimics human rating process for more consistent results"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def create_structured_prompt(review_text: str) -> str:\n",
                "    \"\"\"Structured criteria-based analysis approach.\"\"\"\n",
                "    return f\"\"\"You are a professional review analyst. Evaluate this Yelp review using a structured framework.\n",
                "\n",
                "Review: \"{review_text}\"\n",
                "\n",
                "Rating Guidelines:\n",
                "★★★★★ (5 stars): Exceptional experience, highly enthusiastic, strong recommendation, minimal/no complaints\n",
                "★★★★☆ (4 stars): Very good experience, mostly positive, minor issues mentioned, would likely return\n",
                "★★★☆☆ (3 stars): Average/mixed experience, balanced positives and negatives, neutral stance\n",
                "★★☆☆☆ (2 stars): Below average, more negatives than positives, disappointed, unlikely to return\n",
                "★☆☆☆☆ (1 star): Terrible experience, extremely negative, strong complaints, warns others away\n",
                "\n",
                "Evaluation Criteria:\n",
                "1. Sentiment Analysis: What is the dominant emotional tone?\n",
                "2. Language Intensity: Are superlatives or extreme words used?\n",
                "3. Specific Feedback: What concrete details are mentioned (food quality, service, atmosphere, value)?\n",
                "4. Recommendation Likelihood: Would the reviewer recommend this to others?\n",
                "5. Complaint Severity: How serious are any issues mentioned?\n",
                "\n",
                "Return ONLY a JSON object:\n",
                "{{\n",
                "  \"predicted_stars\": <number between 1-5>,\n",
                "  \"explanation\": \"<detailed analysis based on criteria>\"\n",
                "}}\n",
                "\"\"\"\n",
                "\n",
                "# Test the prompt\n",
                "print(\"Example Prompt (Approach 4):\")\n",
                "print(\"=\"*80)\n",
                "print(create_structured_prompt(test_review)[:500] + \"...\")\n",
                "print(\"=\"*80)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Helper Functions for API Calls and JSON Parsing"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def extract_json_from_response(response_text: str) -> Optional[Dict]:\n",
                "    \"\"\"Extract JSON from various response formats.\"\"\"\n",
                "    try:\n",
                "        # Try direct JSON parsing\n",
                "        return json.loads(response_text)\n",
                "    except json.JSONDecodeError:\n",
                "        pass\n",
                "    \n",
                "    # Try to find JSON in markdown code blocks\n",
                "    json_pattern = r'```(?:json)?\\s*({.*?})\\s*```'\n",
                "    matches = re.findall(json_pattern, response_text, re.DOTALL)\n",
                "    if matches:\n",
                "        try:\n",
                "            return json.loads(matches[0])\n",
                "        except json.JSONDecodeError:\n",
                "            pass\n",
                "    \n",
                "    # Try to find any JSON object\n",
                "    json_pattern = r'{[^{}]*\"predicted_stars\"[^{}]*}'\n",
                "    matches = re.findall(json_pattern, response_text, re.DOTALL)\n",
                "    if matches:\n",
                "        try:\n",
                "            return json.loads(matches[0])\n",
                "        except json.JSONDecodeError:\n",
                "            pass\n",
                "    \n",
                "    return None\n",
                "\n",
                "\n",
                "def validate_prediction(prediction: Dict) -> bool:\n",
                "    \"\"\"Validate prediction structure and values.\"\"\"\n",
                "    if not isinstance(prediction, dict):\n",
                "        return False\n",
                "    \n",
                "    if 'predicted_stars' not in prediction:\n",
                "        return False\n",
                "    \n",
                "    stars = prediction['predicted_stars']\n",
                "    if not isinstance(stars, (int, float)):\n",
                "        return False\n",
                "    \n",
                "    if not (1 <= stars <= 5):\n",
                "        return False\n",
                "    \n",
                "    return True\n",
                "\n",
                "\n",
                "def call_llm_with_retry(prompt: str, max_retries: int = 3, delay: float = 1.0) -> Tuple[Optional[Dict], str]:\n",
                "    \"\"\"Call LLM with retry logic and return (prediction_dict, raw_response).\"\"\"\n",
                "    for attempt in range(max_retries):\n",
                "        try:\n",
                "            response = model.generate_content(prompt)\n",
                "            raw_response = response.text\n",
                "            \n",
                "            # Extract JSON\n",
                "            prediction = extract_json_from_response(raw_response)\n",
                "            \n",
                "            if prediction and validate_prediction(prediction):\n",
                "                return prediction, raw_response\n",
                "            \n",
                "            # If validation failed, retry\n",
                "            if attempt < max_retries - 1:\n",
                "                time.sleep(delay * (attempt + 1))\n",
                "                continue\n",
                "            \n",
                "            return None, raw_response\n",
                "            \n",
                "        except Exception as e:\n",
                "            if attempt < max_retries - 1:\n",
                "                time.sleep(delay * (attempt + 1))\n",
                "                continue\n",
                "            return None, str(e)\n",
                "    \n",
                "    return None, \"Max retries exceeded\"\n",
                "\n",
                "\n",
                "print(\"✓ Helper functions defined\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Evaluation Function"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def evaluate_approach(approach_name: str, prompt_function, sample_data: pd.DataFrame, \n",
                "                      delay_between_calls: float = 0.5) -> pd.DataFrame:\n",
                "    \"\"\"Evaluate a prompting approach on the sample dataset.\"\"\"\n",
                "    print(f\"\\n{'='*80}\")\n",
                "    print(f\"Evaluating: {approach_name}\")\n",
                "    print(f\"{'='*80}\\n\")\n",
                "    \n",
                "    results = []\n",
                "    total = len(sample_data)\n",
                "    \n",
                "    for idx, row in sample_data.iterrows():\n",
                "        review_text = row['text']\n",
                "        actual_stars = row['stars']\n",
                "        \n",
                "        # Create prompt\n",
                "        prompt = prompt_function(review_text)\n",
                "        \n",
                "        # Call LLM\n",
                "        prediction, raw_response = call_llm_with_retry(prompt)\n",
                "        \n",
                "        # Store results\n",
                "        result = {\n",
                "            'review_text': review_text,\n",
                "            'actual_stars': actual_stars,\n",
                "            'predicted_stars': prediction['predicted_stars'] if prediction else None,\n",
                "            'explanation': prediction.get('explanation', '') if prediction else '',\n",
                "            'raw_response': raw_response,\n",
                "            'json_valid': prediction is not None\n",
                "        }\n",
                "        results.append(result)\n",
                "        \n",
                "        # Progress indicator\n",
                "        if (idx + 1) % 10 == 0 or (idx + 1) == total:\n",
                "            valid_count = sum(1 for r in results if r['json_valid'])\n",
                "            print(f\"Progress: {idx + 1}/{total} | Valid JSON: {valid_count}/{idx + 1} ({100*valid_count/(idx+1):.1f}%)\")\n",
                "        \n",
                "        # Rate limiting\n",
                "        time.sleep(delay_between_calls)\n",
                "    \n",
                "    results_df = pd.DataFrame(results)\n",
                "    \n",
                "    # Calculate metrics\n",
                "    valid_predictions = results_df[results_df['json_valid']]\n",
                "    \n",
                "    if len(valid_predictions) > 0:\n",
                "        accuracy = accuracy_score(valid_predictions['actual_stars'], valid_predictions['predicted_stars'])\n",
                "        mae = mean_absolute_error(valid_predictions['actual_stars'], valid_predictions['predicted_stars'])\n",
                "    else:\n",
                "        accuracy = 0.0\n",
                "        mae = float('inf')\n",
                "    \n",
                "    json_validity_rate = results_df['json_valid'].mean()\n",
                "    \n",
                "    print(f\"\\n{'='*80}\")\n",
                "    print(f\"Results for {approach_name}:\")\n",
                "    print(f\"{'='*80}\")\n",
                "    print(f\"Total Reviews: {len(results_df)}\")\n",
                "    print(f\"Valid JSON Responses: {results_df['json_valid'].sum()} ({100*json_validity_rate:.1f}%)\")\n",
                "    print(f\"Accuracy: {100*accuracy:.2f}%\")\n",
                "    print(f\"Mean Absolute Error: {mae:.3f}\")\n",
                "    print(f\"{'='*80}\\n\")\n",
                "    \n",
                "    return results_df\n",
                "\n",
                "\n",
                "print(\"✓ Evaluation function defined\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 7. Run All Evaluations\n",
                "\n",
                "**Note**: This will take approximately 10-20 minutes depending on API response times.\n",
                "\n",
                "**IMPORTANT**: If you have already run the evaluations and have the CSV files, you can skip this section and jump to Section 8."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Dictionary to store all results\n",
                "all_results = {}\n",
                "\n",
                "# Approach 1: Simple Direct\n",
                "all_results['Simple Direct'] = evaluate_approach(\n",
                "    \"Approach 1: Simple Direct Prompting\",\n",
                "    create_simple_prompt,\n",
                "    sample_df\n",
                ")\n",
                "\n",
                "# Save intermediate results\n",
                "all_results['Simple Direct'].to_csv('results_simple_direct.csv', index=False)\n",
                "print(\"✓ Saved results_simple_direct.csv\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Approach 2: Few-Shot Learning\n",
                "all_results['Few-Shot Learning'] = evaluate_approach(\n",
                "    \"Approach 2: Few-Shot Learning\",\n",
                "    create_fewshot_prompt,\n",
                "    sample_df\n",
                ")\n",
                "\n",
                "# Save intermediate results\n",
                "all_results['Few-Shot Learning'].to_csv('results_fewshot.csv', index=False)\n",
                "print(\"✓ Saved results_fewshot.csv\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Approach 3: Chain-of-Thought\n",
                "all_results['Chain-of-Thought'] = evaluate_approach(\n",
                "    \"Approach 3: Chain-of-Thought (CoT)\",\n",
                "    create_cot_prompt,\n",
                "    sample_df\n",
                ")\n",
                "\n",
                "# Save intermediate results\n",
                "all_results['Chain-of-Thought'].to_csv('results_cot.csv', index=False)\n",
                "print(\"✓ Saved results_cot.csv\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Approach 4: Structured Criteria-Based\n",
                "all_results['Structured Analysis'] = evaluate_approach(\n",
                "    \"Approach 4: Structured Criteria-Based Analysis\",\n",
                "    create_structured_prompt,\n",
                "    sample_df\n",
                ")\n",
                "\n",
                "# Save intermediate results\n",
                "all_results['Structured Analysis'].to_csv('results_structured.csv', index=False)\n",
                "print(\"✓ Saved results_structured.csv\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 8. Load Results from CSV Files\n",
                "\n",
                "**Run this cell if you want to analyze previously generated results without re-running the API calls.**"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load all results from CSV files\n",
                "all_results = {\n",
                "    'Simple Direct': pd.read_csv('results_simple_direct.csv'),\n",
                "    'Few-Shot Learning': pd.read_csv('results_fewshot.csv'),\n",
                "    'Chain-of-Thought': pd.read_csv('results_cot.csv'),\n",
                "    'Structured Analysis': pd.read_csv('results_structured.csv')\n",
                "}\n",
                "\n",
                "print(\"✓ Loaded all results from CSV files\")\n",
                "print(f\"\\nResults summary:\")\n",
                "for approach_name, results_df in all_results.items():\n",
                "    valid_count = results_df['json_valid'].sum()\n",
                "    total = len(results_df)\n",
                "    print(f\"  {approach_name}: {total} reviews, {valid_count} valid ({100*valid_count/total:.1f}%)\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 9. Comparative Analysis"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create comparison table\n",
                "comparison_data = []\n",
                "\n",
                "for approach_name, results_df in all_results.items():\n",
                "    valid_predictions = results_df[results_df['json_valid']]\n",
                "    \n",
                "    if len(valid_predictions) > 0:\n",
                "        accuracy = accuracy_score(valid_predictions['actual_stars'], valid_predictions['predicted_stars'])\n",
                "        mae = mean_absolute_error(valid_predictions['actual_stars'], valid_predictions['predicted_stars'])\n",
                "    else:\n",
                "        accuracy = 0.0\n",
                "        mae = float('inf')\n",
                "    \n",
                "    json_validity = results_df['json_valid'].mean()\n",
                "    \n",
                "    comparison_data.append({\n",
                "        'Approach': approach_name,\n",
                "        'Accuracy (%)': f\"{100*accuracy:.2f}\",\n",
                "        'Mean Absolute Error': f\"{mae:.3f}\",\n",
                "        'JSON Validity Rate (%)': f\"{100*json_validity:.2f}\",\n",
                "        'Valid Predictions': len(valid_predictions),\n",
                "        'Total Reviews': len(results_df)\n",
                "    })\n",
                "\n",
                "comparison_df = pd.DataFrame(comparison_data)\n",
                "comparison_df.to_csv('approach_comparison.csv', index=False)\n",
                "\n",
                "print(\"\\n\" + \"=\"*100)\n",
                "print(\"COMPARISON TABLE: All Approaches\")\n",
                "print(\"=\"*100)\n",
                "print(comparison_df.to_string(index=False))\n",
                "print(\"=\"*100)\n",
                "\n",
                "print(\"\\n✓ Saved approach_comparison.csv\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Visualize comparison\n",
                "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
                "\n",
                "approaches = comparison_df['Approach'].tolist()\n",
                "accuracies = [float(x) for x in comparison_df['Accuracy (%)']]\n",
                "maes = [float(x) for x in comparison_df['Mean Absolute Error']]\n",
                "json_rates = [float(x) for x in comparison_df['JSON Validity Rate (%)']]\n",
                "\n",
                "# Accuracy comparison\n",
                "axes[0].bar(approaches, accuracies, color='steelblue', edgecolor='black')\n",
                "axes[0].set_title('Accuracy Comparison', fontsize=14, fontweight='bold')\n",
                "axes[0].set_ylabel('Accuracy (%)')\n",
                "axes[0].set_ylim(0, 100)\n",
                "axes[0].tick_params(axis='x', rotation=45)\n",
                "for i, v in enumerate(accuracies):\n",
                "    axes[0].text(i, v + 2, f'{v:.1f}%', ha='center', fontweight='bold')\n",
                "\n",
                "# MAE comparison (lower is better)\n",
                "axes[1].bar(approaches, maes, color='coral', edgecolor='black')\n",
                "axes[1].set_title('Mean Absolute Error (Lower is Better)', fontsize=14, fontweight='bold')\n",
                "axes[1].set_ylabel('MAE')\n",
                "axes[1].tick_params(axis='x', rotation=45)\n",
                "for i, v in enumerate(maes):\n",
                "    axes[1].text(i, v + 0.05, f'{v:.2f}', ha='center', fontweight='bold')\n",
                "\n",
                "# JSON validity comparison\n",
                "axes[2].bar(approaches, json_rates, color='mediumseagreen', edgecolor='black')\n",
                "axes[2].set_title('JSON Validity Rate', fontsize=14, fontweight='bold')\n",
                "axes[2].set_ylabel('Validity Rate (%)')\n",
                "axes[2].set_ylim(0, 100)\n",
                "axes[2].tick_params(axis='x', rotation=45)\n",
                "for i, v in enumerate(json_rates):\n",
                "    axes[2].text(i, v + 2, f'{v:.1f}%', ha='center', fontweight='bold')\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.savefig('approach_comparison.png', dpi=300, bbox_inches='tight')\n",
                "plt.show()\n",
                "\n",
                "print(\"✓ Saved approach_comparison.png\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 10. Detailed Analysis of Best Approach"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Find best approach by accuracy\n",
                "best_approach_name = comparison_df.loc[comparison_df['Accuracy (%)'].astype(float).idxmax(), 'Approach']\n",
                "best_results = all_results[best_approach_name]\n",
                "best_valid = best_results[best_results['json_valid']]\n",
                "\n",
                "print(f\"\\nBest Approach: {best_approach_name}\\n\")\n",
                "\n",
                "# Classification report\n",
                "print(\"Classification Report:\")\n",
                "print(\"=\"*80)\n",
                "print(classification_report(\n",
                "    best_valid['actual_stars'], \n",
                "    best_valid['predicted_stars'],\n",
                "    target_names=['1 star', '2 stars', '3 stars', '4 stars', '5 stars']\n",
                "))\n",
                "print(\"=\"*80)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Confusion matrix\n",
                "cm = confusion_matrix(best_valid['actual_stars'], best_valid['predicted_stars'])\n",
                "\n",
                "plt.figure(figsize=(10, 8))\n",
                "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
                "            xticklabels=['1★', '2★', '3★', '4★', '5★'],\n",
                "            yticklabels=['1★', '2★', '3★', '4★', '5★'],\n",
                "            cbar_kws={'label': 'Count'})\n",
                "plt.title(f'Confusion Matrix: {best_approach_name}', fontsize=16, fontweight='bold')\n",
                "plt.xlabel('Predicted Rating', fontsize=12)\n",
                "plt.ylabel('Actual Rating', fontsize=12)\n",
                "plt.tight_layout()\n",
                "plt.savefig('confusion_matrix_best.png', dpi=300, bbox_inches='tight')\n",
                "plt.show()\n",
                "\n",
                "print(\"✓ Saved confusion_matrix_best.png\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 11. Sample Predictions Analysis"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Show some correct predictions\n",
                "correct_predictions = best_valid[best_valid['actual_stars'] == best_valid['predicted_stars']]\n",
                "\n",
                "print(\"\\n\" + \"=\"*100)\n",
                "print(\"SAMPLE CORRECT PREDICTIONS\")\n",
                "print(\"=\"*100)\n",
                "\n",
                "for i, row in correct_predictions.head(3).iterrows():\n",
                "    print(f\"\\nReview: {row['review_text'][:200]}...\")\n",
                "    print(f\"Actual: {row['actual_stars']}★ | Predicted: {row['predicted_stars']}★\")\n",
                "    print(f\"Explanation: {row['explanation']}\")\n",
                "    print(\"-\" * 100)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Show some incorrect predictions\n",
                "incorrect_predictions = best_valid[best_valid['actual_stars'] != best_valid['predicted_stars']]\n",
                "\n",
                "print(\"\\n\" + \"=\"*100)\n",
                "print(\"SAMPLE INCORRECT PREDICTIONS (for analysis)\")\n",
                "print(\"=\"*100)\n",
                "\n",
                "for i, row in incorrect_predictions.head(3).iterrows():\n",
                "    print(f\"\\nReview: {row['review_text'][:200]}...\")\n",
                "    print(f\"Actual: {row['actual_stars']}★ | Predicted: {row['predicted_stars']}★ | Error: {abs(row['actual_stars'] - row['predicted_stars'])}\")\n",
                "    print(f\"Explanation: {row['explanation']}\")\n",
                "    print(\"-\" * 100)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 12. Discussion and Insights\n",
                "\n",
                "### Prompt Evolution and Improvements\n",
                "\n",
                "#### Approach 1 → 2: Adding Examples\n",
                "**Change**: Added 5 diverse examples covering each rating level\n",
                "**Rationale**: Few-shot learning helps calibrate the model's understanding of the rating scale\n",
                "**Expected Impact**: Improved consistency and better boundary detection between ratings\n",
                "\n",
                "#### Approach 2 → 3: Encouraging Reasoning\n",
                "**Change**: Introduced step-by-step thinking process\n",
                "**Rationale**: Chain-of-thought prompting improves performance on complex reasoning tasks\n",
                "**Expected Impact**: Better handling of ambiguous or mixed-sentiment reviews\n",
                "\n",
                "#### Approach 3 → 4: Structured Framework\n",
                "**Change**: Explicit evaluation criteria and detailed rating guidelines\n",
                "**Rationale**: Mimics professional review analysis process\n",
                "**Expected Impact**: Most consistent and accurate predictions, especially for edge cases\n",
                "\n",
                "### Key Findings\n",
                "\n",
                "1. **JSON Validity**: All approaches should achieve >95% validity with proper error handling\n",
                "2. **Accuracy Trends**: More sophisticated prompts generally perform better\n",
                "3. **Trade-offs**: \n",
                "   - Simple prompts: Faster, cheaper, but less accurate\n",
                "   - Complex prompts: More accurate, but higher token cost and latency\n",
                "\n",
                "### Common Challenges\n",
                "\n",
                "1. **Sarcasm Detection**: LLMs may miss sarcastic reviews\n",
                "2. **Mixed Reviews**: Reviews with both positive and negative aspects are harder to rate\n",
                "3. **Context Sensitivity**: Some reviews require domain knowledge (e.g., restaurant vs. service business)\n",
                "4. **Rating Scale Interpretation**: 3-star reviews are most ambiguous\n",
                "\n",
                "### Recommendations\n",
                "\n",
                "- **For Production**: Use Few-Shot or Structured approach for best balance of accuracy and cost\n",
                "- **For Speed**: Simple Direct approach with post-processing\n",
                "- **For Accuracy**: Structured Criteria-Based with ensemble voting\n",
                "\n",
                "### Future Improvements\n",
                "\n",
                "1. **Ensemble Methods**: Combine multiple approaches and vote\n",
                "2. **Fine-tuning**: Train a specialized model on Yelp data\n",
                "3. **Active Learning**: Identify uncertain predictions for human review\n",
                "4. **Domain Adaptation**: Customize prompts for specific business types"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 13. Summary Statistics"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"\\n\" + \"=\"*100)\n",
                "print(\"FINAL SUMMARY\")\n",
                "print(\"=\"*100)\n",
                "print(f\"\\nTotal Reviews Evaluated: {len(sample_df)}\")\n",
                "print(f\"Number of Approaches Tested: {len(all_results)}\")\n",
                "print(f\"\\nBest Approach: {best_approach_name}\")\n",
                "print(f\"Best Accuracy: {comparison_df.loc[comparison_df['Approach'] == best_approach_name, 'Accuracy (%)'].values[0]}%\")\n",
                "print(f\"\\nGenerated Files:\")\n",
                "print(\"  - approach_comparison.csv\")\n",
                "print(\"  - approach_comparison.png\")\n",
                "print(\"  - confusion_matrix_best.png\")\n",
                "print(\"  - results_simple_direct.csv\")\n",
                "print(\"  - results_fewshot.csv\")\n",
                "print(\"  - results_cot.csv\")\n",
                "print(\"  - results_structured.csv\")\n",
                "print(\"\\n\" + \"=\"*100)\n",
                "print(\"✓ Task 1 Complete!\")\n",
                "print(\"=\"*100)"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}